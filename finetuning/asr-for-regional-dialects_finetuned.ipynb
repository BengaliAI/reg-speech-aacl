{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuation of the pseudolabeling pipeline described in https://www.kaggle.com/code/reasat/pseudolabeling-step-1-download-speech-audio\n",
    "\n",
    "Model weight and inference notebook copied from: https://www.kaggle.com/competitions/bengaliai-speech/discussion/447970\n",
    "\n",
    "## STT Model:\n",
    "\n",
    "* OpenAI whisper-medium\n",
    "* Huggingface trainer\n",
    "* Trained on 8x 48GB RTX A6000\n",
    "* bs=8 and lr=1e-5\n",
    "* Train steps 50k\n",
    "* Spectrogram dithering\n",
    "* Spectrogram time and frequency masking\n",
    "* Resampling 16khz->8khz->16khz as augmentation\n",
    "* Inference with max_length=260, num_beams=4 and chunk_length_s=20.1s\n",
    "* Libsonic based speed/pitch augmentation\n",
    "* Datasets: OpenSLR 37, OpenSLR 53, MadASR, Shrutilipi, Macro, Kathbath, GoogleTTS generated audios and pseudo labeled YouTube videos\n",
    "\n",
    "## Punctuation Model:\n",
    "\n",
    "* AutoModelForTokenClassification google/muril-base-cased\n",
    "* Huggingface trainer\n",
    "* Labels: period, comma and question mark\n",
    "* bs=64, lr=2e-4 and max_seq_length=512\n",
    "* Ensemble of 4 models (using 6, 8, 11 and 12 layers of google/muril-base-cased)\n",
    "* Normalized IndicCorp v2 Bangla dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer  # you may need to install this library\n",
    "domain_weights = {\n",
    "        'Barishal': 0.125,\n",
    "        'Chittagong': 0.083,\n",
    "        'Habiganj': 0.125,\n",
    "        'Kishoreganj': 0.083,\n",
    "        'Narail': 0.083, \n",
    "        'Narsingdi': 0.083,\n",
    "        'Rangpur': 0.083,\n",
    "        'Sylhet': 0.125,\n",
    "        'Sandwip': 0.125,\n",
    "        'Tangail': 0.083,\n",
    "    }\n",
    "domain_weights = { key.lower(): value for key, value in domain_weights.items()}\n",
    "# unseen: Habiganj, Barishal, Sylhet, Sandwip\n",
    "def mean_wer(solution, submission):\n",
    "    joined = solution.merge(submission.rename(columns={'sentence': 'predicted'}))\n",
    "#     print(joined)\n",
    "    domain_scores = joined.groupby('domain').apply(\n",
    "        # note that jiwer.wer computes a weighted average wer by default when given lists of strings\n",
    "        lambda df: jiwer.wer(df['sentence'].to_list(), df['predicted'].to_list()),\n",
    "    )\n",
    "    print(domain_scores)\n",
    "    for key, value in domain_weights.items():\n",
    "        domain_scores.loc[key] = domain_scores.loc[key].item()*value\n",
    "    print(domain_scores)\n",
    "    return domain_scores.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "\n",
    "dir_root = '/home/phison/LargeFiles/'\n",
    "\n",
    "# MODEL = '/kaggle/input/bengali-ai-asr-submission/bengali-whisper-medium/'\n",
    "PUNCT_MODELS = [\n",
    "    '/home/phison/LargeFiles/bengali-ai-asr-submission/punct-model-6layers/',\n",
    "    '/home/phison/LargeFiles/bengali-ai-asr-submission/punct-model-8layers/',\n",
    "    '/home/phison/LargeFiles/bengali-ai-asr-submission/punct-model-11layers/',\n",
    "    '/home/phison/LargeFiles/bengali-ai-asr-submission/punct-model-12layers/'\n",
    "]\n",
    "PUNCT_WEIGHTS = [[1.0, 1.4, 1.0, 0.8]]\n",
    "\n",
    "CHUNK_LENGTH_S = 20.1\n",
    "ENABLE_BEAM = True\n",
    "\n",
    "\n",
    "\n",
    "if ENABLE_BEAM:\n",
    "    BATCH_SIZE = 32*8\n",
    "else:\n",
    "    BATCH_SIZE = 32*8\n",
    "\n",
    "DATASET_PATH = '/home/phison/LargeFiles/iut-comp-dataset/16_kHz_test_audio'\n",
    "# MODEL = 'BengaliAI/tugstugi_bengaliai-asr_whisper-medium'\n",
    "MODEL = '/home/phison/LargeFiles/regional-asr/whisper-base-bn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.2\n",
      "              file_name                                        transcripts  \\\n",
      "0  test_sandwip (1).wav  হুম, আই দাওয়াত খাইবো। খাইবো অনকা তো বুজি <> হব...   \n",
      "1  test_sandwip (2).wav  এন্নে বিয়া-সাদি চুয়াইছে, ঘরে সংসারও চইলছে, হুম...   \n",
      "2  test_sandwip (3).wav  হুম, হাক্কন হিডা ন নি হারা যাইবো নে? এক্কবারে ...   \n",
      "3  test_sandwip (4).wav  আম-কাডল <> ধর ইয়া আছে, হল আছে তারফরে দুধ আছে আ...   \n",
      "4  test_sandwip (5).wav  এই তুই গোসল কইত্তি ন? হিয়া হরে কইরবো এরি। গোসল...   \n",
      "\n",
      "  district  \n",
      "0  sandwip  \n",
      "1  sandwip  \n",
      "2  sandwip  \n",
      "3  sandwip  \n",
      "4  sandwip  \n",
      "files 1700\n",
      "['/home/phison/LargeFiles/iut-comp-dataset/16_kHz_test_audio/test_sandwip (1).wav', '/home/phison/LargeFiles/iut-comp-dataset/16_kHz_test_audio/test_sandwip (2).wav', '/home/phison/LargeFiles/iut-comp-dataset/16_kHz_test_audio/test_sandwip (3).wav']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import glob\n",
    "import shutil\n",
    "import librosa\n",
    "import argparse\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# files = []\n",
    "df_sub = pd.read_csv(dir_root+'/iut-comp-dataset/test.csv')\n",
    "print(df_sub.head())\n",
    "df_sub['paths'] = df_sub['file_name'].apply(lambda x: os.path.join(DATASET_PATH, x))\n",
    "files = df_sub['paths'].to_list()\n",
    "# files += list(glob.glob(DATASET_PATH + '/' + '*.mp3'))\n",
    "print('files', len(files))\n",
    "print(files[:3])\n",
    "# NOTE: running on a few samples for demonstration\n",
    "# files = files[:10]\n",
    "\n",
    "# files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=\"automatic-speech-recognition\",\n",
    "                model=MODEL,\n",
    "                tokenizer=MODEL,\n",
    "                chunk_length_s=CHUNK_LENGTH_S, device=0, \n",
    "#                 batch_size=BATCH_SIZE\n",
    "               )\n",
    "pipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=\"bn\", task=\"transcribe\")\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_repetition(text, max_count):\n",
    "    uniq_word_counter = {}\n",
    "    words = text.split()\n",
    "    for word in text.split():\n",
    "        if word not in uniq_word_counter:\n",
    "            uniq_word_counter[word] = 1\n",
    "        else:\n",
    "            uniq_word_counter[word] += 1\n",
    "\n",
    "    for word, count in uniq_word_counter.items():\n",
    "        if count > max_count:\n",
    "            words = [w for w in words if w != word]\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed: 256, avg. sec/sample: 2.79\n",
      "completed: 512, avg. sec/sample: 2.70\n",
      "completed: 768, avg. sec/sample: 2.66\n",
      "completed: 1024, avg. sec/sample: 2.63\n",
      "completed: 1280, avg. sec/sample: 2.58\n",
      "completed: 1536, avg. sec/sample: 2.57\n",
      "completed: 1700, avg. sec/sample: 2.60\n",
      "total time: 4413.94\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import time\n",
    "def batchify(inputs, batch_size):\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        yield inputs[i:i + batch_size]\n",
    "    \n",
    "# ENABLE_BEAM = 0\n",
    "generate_kwargs = {\"max_length\": 260, \"num_beams\": 4} if ENABLE_BEAM else None\n",
    "start = time.time()\n",
    "texts = []\n",
    "for batch in batchify(files, BATCH_SIZE):\n",
    "    texts+=pipe(batch, generate_kwargs = generate_kwargs)\n",
    "    elapsed = time.time()-start\n",
    "    print('completed: {}, avg. sec/sample: {:.2f}'.format(len(texts), elapsed/len(texts)))\n",
    "print('total time: {:.2f}'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del pipe\n",
    "import torch\n",
    "models = [\n",
    "    AutoModelForTokenClassification.from_pretrained(f).eval().cuda() for f in PUNCT_MODELS\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(PUNCT_MODELS[0])\n",
    "def punctuate(text):\n",
    "    input_ids = tokenizer(text).input_ids\n",
    "    with torch.no_grad():\n",
    "        model = models[0]\n",
    "        logits = torch.nn.functional.softmax(\n",
    "            model(input_ids=torch.LongTensor([input_ids]).cuda()).logits[0, 1:-1],\n",
    "            dim=1).cpu()\n",
    "        for model in models[1:]:\n",
    "            logits += torch.nn.functional.softmax(\n",
    "                model(input_ids=torch.LongTensor([input_ids]).cuda()).logits[0, 1:-1],\n",
    "                dim=1).cpu()\n",
    "        logits = logits / len(models)\n",
    "        logits *= torch.FloatTensor(PUNCT_WEIGHTS)\n",
    "        label_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        tokens = tokenizer(text, add_special_tokens=False).input_ids\n",
    "        punct_text = \"\"\n",
    "        for index, token in enumerate(tokens):\n",
    "            token_str = tokenizer.decode(token)\n",
    "            if '##' not in token_str:\n",
    "                punct_text += \" \" + token_str\n",
    "            else:\n",
    "                punct_text += token_str[2:]\n",
    "            punct_text += ['', '।', ',', '?'][label_ids[index].item()]\n",
    "\n",
    "    punct_text = punct_text.strip()\n",
    "    return punct_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [{'text': 'text'} for _ in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference finished!\n",
      "          file_name                                           sentence\n",
      "0  test_sandwip (1)                                        হুম। খাইবো�\n",
      "1  test_sandwip (2)  <> ঘরের সংসারে চলছে। হুম। বেশি থাকলে বেশি খরচ ...\n",
      "2  test_sandwip (3)  হুম। হাক্কুনিডা নোনি হারো যাইবানে এক্কবারে, এই...\n",
      "3  test_sandwip (4)  আম কাডোলা এই দরো ইয়া আছে, হল আছে। তারপরে, দুধ ...\n",
      "4  test_sandwip (5)  এই তো গোসল করতি না? হিয়া হরে কইরবুরি? হুম। গোস...\n",
      "          file_name                                           sentence  \\\n",
      "0  test_sandwip (1)  হুম, আই দাওয়াত খাইবো। খাইবো অনকা তো বুজি <> হব...   \n",
      "1  test_sandwip (2)  এন্নে বিয়া-সাদি চুয়াইছে, ঘরে সংসারও চইলছে, হুম...   \n",
      "2  test_sandwip (3)  হুম, হাক্কন হিডা ন নি হারা যাইবো নে? এক্কবারে ...   \n",
      "3  test_sandwip (4)  আম-কাডল <> ধর ইয়া আছে, হল আছে তারফরে দুধ আছে আ...   \n",
      "4  test_sandwip (5)  এই তুই গোসল কইত্তি ন? হিয়া হরে কইরবো এরি। গোসল...   \n",
      "\n",
      "    domain  \n",
      "0  sandwip  \n",
      "1  sandwip  \n",
      "2  sandwip  \n",
      "3  sandwip  \n",
      "4  sandwip  \n",
      "domain\n",
      "barishal       0.672002\n",
      "chittagong     0.848672\n",
      "habiganj       0.654070\n",
      "kishoreganj    0.864722\n",
      "narail         0.618106\n",
      "narsingdi      0.641235\n",
      "rangpur        0.776455\n",
      "sandwip        0.816957\n",
      "sylhet         0.750967\n",
      "tangail        0.364967\n",
      "dtype: float64\n",
      "domain\n",
      "barishal       0.084000\n",
      "chittagong     0.070440\n",
      "habiganj       0.081759\n",
      "kishoreganj    0.071772\n",
      "narail         0.051303\n",
      "narsingdi      0.053222\n",
      "rangpur        0.064446\n",
      "sandwip        0.102120\n",
      "sylhet         0.093871\n",
      "tangail        0.030292\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7032243544529495"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "with open(\"submission.csv\", 'wt', encoding=\"utf8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['file_name', 'sentence'])\n",
    "    for f, text in zip(files, texts):\n",
    "        file_id = Path(f).stem\n",
    "        pred = text['text'].strip()\n",
    "        pred = fix_repetition(pred, max_count=8)\n",
    "        if len(pred) == 0:\n",
    "            print('empty prediction on', f)\n",
    "            pred = ' '\n",
    "    \n",
    "        prediction = [file_id, pred]\n",
    "        writer.writerow(prediction)\n",
    "        predictions.append(prediction)\n",
    "print(\"inference finished!\")\n",
    "\n",
    "import pandas as pd\n",
    "submission = pd.read_csv(\"submission.csv\")\n",
    "print(submission.head())\n",
    "\n",
    "solution = solution.rename(columns = {'transcripts': 'sentence', 'district': 'domain'}).drop(columns = 'paths')\n",
    "solution['file_name'] = solution['file_name'].apply(lambda x: x.replace('.wav', ''))\n",
    "print(solution.head())\n",
    "\n",
    "mean_wer(solution, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference finished!\n",
      "          file_name                                           sentence\n",
      "0  test_sandwip (1)                                       হুম । খাইবো।\n",
      "1  test_sandwip (2)  < > ঘরের সংসারে চলছে । হুম । বেশি থাকলে বেশি খ...\n",
      "2  test_sandwip (3)  হুম । হাক্কুনিডা নোনি হারো যাইবানে এক্কবারে ,।...\n",
      "3  test_sandwip (4)  আম কাডোলা এই দরো ইয়া আছে , হল আছে । তারপরে , দ...\n",
      "4  test_sandwip (5)  এই তো গোসল করতি না ? হিয়া হরে কইরবুরি ? হুম । ...\n",
      "          file_name                                           sentence  \\\n",
      "0  test_sandwip (1)  হুম, আই দাওয়াত খাইবো। খাইবো অনকা তো বুজি <> হব...   \n",
      "1  test_sandwip (2)  এন্নে বিয়া-সাদি চুয়াইছে, ঘরে সংসারও চইলছে, হুম...   \n",
      "2  test_sandwip (3)  হুম, হাক্কন হিডা ন নি হারা যাইবো নে? এক্কবারে ...   \n",
      "3  test_sandwip (4)  আম-কাডল <> ধর ইয়া আছে, হল আছে তারফরে দুধ আছে আ...   \n",
      "4  test_sandwip (5)  এই তুই গোসল কইত্তি ন? হিয়া হরে কইরবো এরি। গোসল...   \n",
      "\n",
      "    domain  \n",
      "0  sandwip  \n",
      "1  sandwip  \n",
      "2  sandwip  \n",
      "3  sandwip  \n",
      "4  sandwip  \n",
      "domain\n",
      "barishal       0.849765\n",
      "chittagong     1.052656\n",
      "habiganj       0.919816\n",
      "kishoreganj    1.136565\n",
      "narail         0.816852\n",
      "narsingdi      0.868524\n",
      "rangpur        1.008088\n",
      "sandwip        0.970285\n",
      "sylhet         1.018893\n",
      "tangail        0.523829\n",
      "dtype: float64\n",
      "domain\n",
      "barishal       0.106221\n",
      "chittagong     0.087370\n",
      "habiganj       0.114977\n",
      "kishoreganj    0.094335\n",
      "narail         0.067799\n",
      "narsingdi      0.072087\n",
      "rangpur        0.083671\n",
      "sandwip        0.121286\n",
      "sylhet         0.127362\n",
      "tangail        0.043478\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9185856242790656"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "with open(\"submission.csv\", 'wt', encoding=\"utf8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['file_name', 'sentence'])\n",
    "    for f, text in zip(files, texts):\n",
    "        file_id = Path(f).stem\n",
    "        pred = text['text'].strip()\n",
    "        pred = fix_repetition(pred, max_count=8)\n",
    "        if len(pred) == 0:\n",
    "            print('empty prediction on', f)\n",
    "            pred = ' '\n",
    "        else:\n",
    "            # pass\n",
    "            pred = punctuate(pred)\n",
    "            if pred[-1] not in ['।', '?', ',']:\n",
    "                pred = pred + '।'\n",
    "        # print(i, file_id, pred)\n",
    "        prediction = [file_id, pred]\n",
    "        writer.writerow(prediction)\n",
    "        predictions.append(prediction)\n",
    "print(\"inference finished!\")\n",
    "\n",
    "import pandas as pd\n",
    "submission = pd.read_csv(\"submission.csv\")\n",
    "print(submission.head())\n",
    "\n",
    "solution = pd.read_csv(dir_root+'/iut-comp-dataset/test.csv')\n",
    "solution = solution.rename(columns = {'transcripts': 'sentence', 'district': 'domain'})\n",
    "solution['file_name'] = solution['file_name'].apply(lambda x: x.replace('.wav', ''))\n",
    "print(solution.head())\n",
    "\n",
    "mean_wer(solution, submission)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2447262,
     "sourceId": 4143520,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3865741,
     "sourceId": 6707460,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4879926,
     "sourceId": 8310846,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
